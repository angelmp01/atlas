# Gu√≠a de Experimentaci√≥n con Hiperpar√°metros XGBoost

Esta gu√≠a muestra c√≥mo usar los nuevos argumentos CLI para experimentar con diferentes configuraciones de XGBoost y aumentar la capacidad del modelo.

## üéØ Objetivo: Aumentar Capacidad del Modelo

Los modelos actuales (~30 MB) pueden tener capacidad limitada para capturar toda la entrop√≠a del dataset. Podemos aumentar la capacidad mediante:

1. **M√°s √°rboles** (`--n-estimators`)
2. **√Årboles m√°s profundos** (`--max-depth`)
3. **Mayor precisi√≥n del histograma** (`--max-bin`)

## üìä Experimentos Recomendados

### Experimento 1: Baseline (Ver Par√°metros Actuales)
```bash
python train.py --show-xgb-params
```

### Experimento 2: Aumentar Capacidad Moderadamente
**Objetivo**: Modelo ~50-60 MB

```bash
python train.py --max-depth 12 --n-estimators 800 --max-bin 384
```

**Cambios**:
- `max_depth`: 10 ‚Üí 12 (+20% profundidad)
- `n_estimators`: 500 ‚Üí 800 (+60% √°rboles)
- `max_bin`: 256 ‚Üí 384 (+50% precisi√≥n)

### Experimento 3: Aumentar Capacidad Agresivamente
**Objetivo**: Modelo ~100-120 MB

```bash
python train.py --max-depth 15 --n-estimators 1500 --max-bin 512
```

**Cambios**:
- `max_depth`: 10 ‚Üí 15 (+50% profundidad)
- `n_estimators`: 500 ‚Üí 1500 (+200% √°rboles)
- `max_bin`: 256 ‚Üí 512 (+100% precisi√≥n)

### Experimento 4: Modelo de Alta Capacidad
**Objetivo**: Modelo ~200+ MB (m√°xima capacidad)

```bash
python train.py --max-depth 20 --n-estimators 2500 --max-bin 1024
```

**Cambios**:
- `max_depth`: 10 ‚Üí 20 (+100% profundidad)
- `n_estimators`: 500 ‚Üí 2500 (+400% √°rboles)
- `max_bin`: 256 ‚Üí 1024 (+300% precisi√≥n)

**‚ö†Ô∏è ADVERTENCIA**: Entrenamiento muy lento (~2-3 horas)

### Experimento 5: Learning Rate Bajo + Muchos √Årboles
**Objetivo**: Aprendizaje m√°s fino y estable

```bash
python train.py --learning-rate 0.02 --n-estimators 2000 --max-depth 12
```

**Ventajas**:
- Aprendizaje m√°s gradual
- Mejor generalizaci√≥n
- Menos overfitting

## üéÆ Quick Test para Validaci√≥n R√°pida

Antes de entrenar el dataset completo, valida configuraciones con `--quick-test`:

```bash
# Test r√°pido con alta capacidad
python train.py --quick-test --only probability --max-depth 15 --n-estimators 1000

# Test r√°pido con regularizaci√≥n
python train.py --quick-test --only probability --subsample 0.6 --colsample-bytree 0.6
```

## üîç An√°lisis de Resultados en Wandb

Todos los experimentos se loguean autom√°ticamente a Wandb con:

1. **Tags**:
   - `custom-hyperparams`: Indica que usaste par√°metros CLI
   - Ver tags en wandb para filtrar experimentos

2. **Config**:
   - Todos los par√°metros XGBoost
   - `cli_overrides`: Qu√© par√°metros cambiaste

3. **M√©tricas**:
   - Compara MAE, RMSE, R¬≤ entre configuraciones
   - Tama√±o del modelo en MB
   - Tiempo de entrenamiento

## üìà Estrategia de B√∫squeda

### Fase 1: B√∫squeda Gruesa (Quick Test)
```bash
# Probar 3-4 configuraciones diferentes con quick-test
python train.py --quick-test --only probability --max-depth 12 --n-estimators 800
python train.py --quick-test --only probability --max-depth 15 --n-estimators 1000
python train.py --quick-test --only probability --max-depth 18 --n-estimators 1500
```

### Fase 2: Refinamiento (Dataset Completo)
```bash
# Entrenar las 1-2 mejores configuraciones en dataset completo
python train.py --max-depth 15 --n-estimators 1200 --max-bin 512
```

### Fase 3: Fine-tuning
```bash
# Ajustar learning rate y regularizaci√≥n
python train.py --max-depth 15 --n-estimators 1500 --learning-rate 0.03 --subsample 0.75
```

## üéØ Combinaciones Recomendadas

### Para Evitar Overfitting
```bash
python train.py --max-depth 12 --min-child-weight 3 --subsample 0.7 --colsample-bytree 0.7
```

### Para M√°xima Capacidad
```bash
python train.py --max-depth 18 --n-estimators 2000 --max-bin 768 --learning-rate 0.03
```

### Balanceado (Recomendado para empezar)
```bash
python train.py --max-depth 13 --n-estimators 1000 --max-bin 384 --learning-rate 0.04
```

## üìù Notas

- **Tiempo de entrenamiento**: Aumenta ~linealmente con `n_estimators`, ~exponencialmente con `max_depth`
- **Memoria GPU**: Modelos m√°s grandes requieren m√°s VRAM
- **Tama√±o del modelo**: Aproximadamente proporcional a `n_estimators * max_depth`
- **Wandb Dashboard**: https://wandb.ai/pgaitl/atlas-ml

## ‚ö° Quick Reference

| Par√°metro | Default | Rango | Efecto en Capacidad | Efecto en Tiempo |
|-----------|---------|-------|---------------------|------------------|
| `max_depth` | 10 | 1-50 | ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è Alto | ‚¨ÜÔ∏è‚¨ÜÔ∏è Exponencial |
| `n_estimators` | 500 | 10-10000 | ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è Alto | ‚¨ÜÔ∏è Lineal |
| `learning_rate` | 0.05 | 0.001-0.3 | ‚û°Ô∏è Neutral | ‚û°Ô∏è Neutral |
| `max_bin` | 256 | 16-1024 | ‚¨ÜÔ∏è Medio | ‚¨ÜÔ∏è Bajo |
| `subsample` | 0.8 | 0.1-1.0 | ‚¨áÔ∏è Baja | ‚¨áÔ∏è Baja |
| `colsample_bytree` | 0.8 | 0.1-1.0 | ‚¨áÔ∏è Baja | ‚¨áÔ∏è Baja |
| `min_child_weight` | 1 | 0-100 | ‚¨áÔ∏è Baja | ‚û°Ô∏è Neutral |
